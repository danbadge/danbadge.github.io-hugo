<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Daniel Watts</title>
    <link>https://danbadge.github.io/post/index.xml</link>
    <description>Recent content in Post-rsses on Daniel Watts</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 29 Dec 2016 16:51:00 +0000</lastBuildDate>
    <atom:link href="https://danbadge.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How we do production release announcements at Driftrock</title>
      <link>https://danbadge.github.io/post/release-notes-at-driftrock/</link>
      <pubDate>Thu, 29 Dec 2016 16:51:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/release-notes-at-driftrock/</guid>
      <description>&lt;p&gt;At Driftrock we&amp;rsquo;re a small company that has grown from 8 to 18 people in the last 9 months. That&amp;rsquo;s not excessive but it still challenged some existing methods of communication. One of those was how we communicate new features which have recently gone live, somewhere along the way we stopped doing this.
&lt;/p&gt;

&lt;h2 id=&#34;shhh-we-re-releasing-software&#34;&gt;Shhh! We&amp;rsquo;re Releasing Software&lt;/h2&gt;

&lt;p&gt;Previously, we deployed a change to one of our applications and didn&amp;rsquo;t tell anyone unless it solved a particular client issue. This meant we wouldn&amp;rsquo;t seek much feedback on new features and the team&amp;rsquo;s progress and activity was largely unseen by the rest of the company. Bearing in mind that our software is increasingly used internally, this seemed like a real missed opportunity.&lt;/p&gt;

&lt;h2 id=&#34;enter-release-crab&#34;&gt;Enter Release Crab&lt;/h2&gt;

&lt;p&gt;Now we have a dedicated Slack channel for internally announcing the release of new features, bug fixes and other changes. The announcement is typically done by a developer after they push their changes to production as they have the most context. But we&amp;rsquo;re pragmatic with it and don&amp;rsquo;t announce every little change, for example, we might wait until we&amp;rsquo;ve finished a sequence of quick deployments. However, &lt;a href=&#34;http://tech.driftrock.com/post/tech-values&#34;&gt;we value feedback&lt;/a&gt; so we&amp;rsquo;re regularly announcing updates.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of a few changes going live for Create (an application for quickly creating ads on Facebook):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://danbadge.github.io/img/release-notes.png&#34; alt=&#34;Example release notes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(Note the crab emoji - this is now synonymous with release announcements)&lt;/p&gt;

&lt;p&gt;This seems like a simple and obvious addition to a production deployment but has had surprising impact, here&amp;rsquo;s a few improvements it&amp;rsquo;s made:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tighter feedback loop&lt;/strong&gt; - more communication between the Product Development team (us) and the other teams in Driftrock - Performance Marketing (internal users), Sales and Client Solutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased accountability and ownership&lt;/strong&gt; - we could automate these release notes but the personal touch adds an element of accountability, giving the readers a point of contact to field any feedback. Therefore the author needs to understand what&amp;rsquo;s going live and (if necessary) what&amp;rsquo;s happening across the team. Gathering this information is required to help the author distil the announcement into something which is simple and concise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keeps deployments small&lt;/strong&gt; - similarly that small amount of manual overhead for the author gives us yet another reason to keep the batch size of a deployment small and ensure we&amp;rsquo;re releasing software early and often.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;future-releases&#34;&gt;Future Releases&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re still getting used to this process and it will certainly evolve. Early ideas to build on this include automating release notes based on commit messages and working out how we can provide our clients with a similar version, possibly even making these public.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Update 09/01/2017: Public facing release notes are now available. They are updated every two weeks with the help of our Client Solutions team and you can find them here - &lt;a href=&#34;http://knowledge.driftrock.com/product-updates&#34;&gt;http://knowledge.driftrock.com/product-updates&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Enabling change on the 7digital Catalogue</title>
      <link>https://danbadge.github.io/post/enabling-change-on-the-7digital-catalogue/</link>
      <pubDate>Tue, 26 Aug 2014 16:51:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/enabling-change-on-the-7digital-catalogue/</guid>
      <description>&lt;p&gt;Since February 2014 I’ve been working on the Content Discovery Team here at 7digital, we are responsible for every endpoint listed under Catalogue API here - &lt;a href=&#34;http://developer.7digital.com/resources/api-docs/catalogue-api&#34;&gt;http://developer.7digital.com/resources/api-docs/catalogue-api&lt;/a&gt; – and probably a few more. This is a mix of discovery endpoints and core catalogue endpoints often split by artist, albums (releases) and tracks. The discovery side of it consists of searching, recommendations, charts, genres, new and popular tracks and albums. The core catalogue endpoints are requests to the API for an album, tracks on an album, an artist, an artist’s releases or just track information using a 7digital unique identifier (referred to as releaseId, trackId and artistId).&lt;/p&gt;

&lt;p&gt;Going back to early February the team, three developers, a QA and product manager, had &lt;strong&gt;inherited a maintenance nightmare&lt;/strong&gt;. We owned around 20 endpoints across 4 applications all of which do vaguely similar things retrieving artist, release or track information. Except we were using &lt;strong&gt;two large out-of-sync data stores&lt;/strong&gt;, a SQL Server database and a Solr data store. Solr essentially containing a denormalised version of what you would find in SQL.&lt;/p&gt;

&lt;p&gt;Over the years there had been a push to use Solr as the single point of truth and there had been much investment to move all endpoints over to use it instead of SQL (a reasonable idea at the time). This meant we were using Solr for both text searching and Id lookups on most of the Catalogue endpoints but using SQL on a number of others. Furthermore we had to index all possible information relating to a track or release in Solr, much of which is territory specific. As an example; the price for an album is territory specific because it will be different in the UK to the US, as will the streaming and download release dates. In order to represent this data in a denormalised form in Solr, the solution was to &lt;strong&gt;duplicate every track and release across every territory it was available in&lt;/strong&gt;. This was a particular problem for the track index. Given we have approximately 27 million tracks in our catalogue and content in 30 odd territories, this quickly led to a massive and expensive index of tracks which was pushing &lt;strong&gt;660 GB in size and contained 700 million documents&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Due to the size of the index and the amount of data the &lt;a href=&#34;http://api.7digital.com/1.2/track/search?q=weezer&amp;amp;oauth_consumer_key=YOUR_KEY_HERE&#34;&gt;~/track/search&lt;/a&gt; endpoint performed horribly averaging 5 second response times. More pressing was the amount of &lt;strong&gt;unplanned and support-type work the team were performing in order to maintain the track index&lt;/strong&gt;. This would vary from restarting live web servers to performing manual data updates to inactivate a track, which should no longer be available on the API. The latter was common because we couldn’t perform a full re-index as it would take up to FIVE DAYS and usually failed, so we built a complex indexing application which would only send to Solr those tracks which had changed. Unfortunately this workaround was built upon an existing process which was used to identify changes in the catalogue. This consisted of tons of triggers and scheduled jobs in SQL Server where it wasn’t uncommon to see nested views, temporary tables and layers of stored procedures. &lt;strong&gt;None of it was tested and all of it was impossible to follow and reason about.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eventually a new feature came along which would have required many changes and additions at each step of this convoluted process. This gave us the business case required to begin simplifying our architecture. We recognised that we needed Solr for searching because this is exactly what it’s good at, but we saw no benefit in using it for Id lookups, particularly when we could be closer to the source data and have good performance using SQL Server. We wanted to shrink the size of the track index and so we needed to tackle a number of problems. We needed to remove territory specific data from the index so we could stop duplicating each track across 30 territories. We also needed to replace any unnecessary dependencies where possible and reduce the number of indexed fields to only those which should be searchable.&lt;/p&gt;

&lt;p&gt;To do any of this we needed to source much of the data stored in Solr from somewhere else. We recognised early on that endpoints such as ~/track/search and ~/track/chart where just a list of tracks and a full track resource could be retrieved from another public endpoint; &lt;a href=&#34;http://api.7digital.com/1.2/track/details?trackid=12345&amp;amp;oauth_consumer_key=YOUR_KEY_HERE&amp;amp;country=GB&#34;&gt;~/track/details&lt;/a&gt;. With this in mind we set about improving a number of core catalogue endpoints (including ~/track/details) reducing response times dramatically, improving the code base and bringing the monitoring of these endpoints up to scratch with everything else. The ~/track/search endpoint could then look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://danbadge.github.io/img/track-search-architecture.jpg&#34; alt=&#34;Track Search Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It remained on the old track index for a while but was only touching a few of the fields. Taking this incremental step, which had a small negative impact on performance, meant we could bring a new smaller index up and switch over to it with minimal disruption. We managed this risk further by using &lt;a href=&#34;http://www.infoq.com/news/2013/03/canary-release-improve-quality&#34;&gt;canary deploys&lt;/a&gt; too, rolling out searches on the new index to a few servers until we were satisfied enough to do a full deployment. The &lt;strong&gt;new track index weighed in at just over 6GB&lt;/strong&gt; and response times are averaging 500ms, a dramatic improvement.&lt;/p&gt;

&lt;p&gt;We can now do a &lt;strong&gt;full re-index of tracks within an hour&lt;/strong&gt;, we do this twice a day. This allows us to experiment more freely using features of Solr and Lucene without being held back by technical debt. Now we make changes daily, which allows us to test out any assumptions. This is particularly valuable when working on something as complex and subjective as text search.&lt;/p&gt;

&lt;p&gt;Crucially with the old track index gone &lt;strong&gt;unplanned work tailed off&lt;/strong&gt; and we started delivering business value. Most recently adding auto-complete style searching to all three search endpoints and resolving some long standing problems with matching on artist names like P!nk and Ke$ha.&lt;/p&gt;

&lt;p&gt;We’ve also taken all the release endpoints in a similar architectural direction, which will eventually mean we can iterate quickly on improving ~/release/search too. Technically we now have only one integration point with the catalogue database for releases and tracks, within ~/release/details and ~/track/details. Now that many of our APIs consume these endpoints we’ve improved consistency between them and we no longer have many tentacles going direct to the database, which will allow us to move a lot quicker and make improvements across the catalogue when needed.&lt;/p&gt;

&lt;p&gt;In conclusion, we benefited from a number of key concepts: we simplified and refactored when working through each of these technical changes, &lt;strong&gt;removing a lot of unnecessary complexity.&lt;/strong&gt; For example, approaching clients about removing rarely used optional parameters and in time removing them.&lt;/p&gt;

&lt;p&gt;We tackled causes of unplanned work, often eliminating any immediate burden with small fixes and workarounds so we could &lt;strong&gt;focus more of the team’s energy on the wider problem&lt;/strong&gt;. Similarly, we elevated these issues, making them visible to the rest of the business. This helped justify our business case for tackling the wider problem.&lt;/p&gt;

&lt;p&gt;We &lt;strong&gt;automated the Solr infrastructure,&lt;/strong&gt; this hasn’t been mentioned much as it’s still evolving, but we’re now able to push through configuration changes to production several times a day. This a world away from where we were and came about through a combination of configuration management (CFEngine), TDD (Test Kitchen, Vagrant), a few custom scripts to automate &lt;a href=&#34;https://wiki.apache.org/solr/CoreAdmin#RELOAD&#34;&gt;Solr core reloads&lt;/a&gt; and reducing the size of the track index.&lt;/p&gt;

&lt;p&gt;Finally, we benefited most from lots of collaboration and communication inside and outside of the team, &lt;strong&gt;discussing the technical and architectural direction&lt;/strong&gt; at length. We even started organising weekly discussions just to run through the architecture we were heading towards, any tweaks we could make or any problems we would encounter along the way. This architecture is continuing to evolve but it’s already paying off, with the team beginning to prove that we can deliver changes quickly to the 7digital catalogue.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For a more detailed and technical view on what we did and how we did it, check out Michael Okarimia’s posts: &lt;a href=&#34;http://www.michaelokarimia.com/blog/2014/08/11/fixing-7digitals-music-search-part-one-the-gargantuan-index/&#34;&gt;part one&lt;/a&gt; and &lt;a href=&#34;http://www.michaelokarimia.com/blog/2014/08/18/fixing-7digital-music-search-part-two-the-88-speed-improvement/&#34;&gt;part two&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ten deploys a day and still moving slowly</title>
      <link>https://danbadge.github.io/post/ten-deploys-a-day-and-still-moving-slowly/</link>
      <pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/ten-deploys-a-day-and-still-moving-slowly/</guid>
      <description>

&lt;p&gt;There’s a feeling within my team at &lt;a href=&#34;http://www.7digital.com/&#34;&gt;7digital&lt;/a&gt; that we’re taking a long time to complete projects or features. Oddly this isn’t something we are currently measuring. We do though measure the &lt;a href=&#34;http://blog.robbowley.net/2012/01/06/productivity-throughput-and-cycle-time/&#34;&gt;throughput and cycle time&lt;/a&gt; of tasks. Tasks are items of work which the team create as a small iterative step towards a larger feature or story. This is useful for visualising the teams productivity, with the idealistic target of low cycle time and high throughput.&lt;/p&gt;

&lt;p&gt;Typically our stakeholders are interested in an estimation and delivery of a project or feature not a task. I was interested to see whether this was something we could measure, with a slant towards having historical data available for helping us estimate. I gathered all the tasks into a spreadsheet going back to the beginning of September 2012 and assigned a descriptive project and feature name I hoped the rest of the organisation could understand. This had the added benefit of forcing me to step back from the implementation and attempt to relate technical items to organisation priorities, something which is far too often overlooked.&lt;/p&gt;

&lt;p&gt;This produced some interesting results, which aligned with the team’s fears. Our average lead time of the six planned projects we had completed since September was 160 days. Over half a year! Moreover, this only represents the time it takes for an organisation-wide project to pass-through our team, often there are several teams involved each with their own changes and features to implement. We do roughly two features per project and so it’s a similar story there. On average we’re taking 72 days to complete a feature.&lt;/p&gt;

&lt;p&gt;We have our &lt;a href=&#34;http://martinfowler.com/bliki/DeploymentPipeline.html&#34;&gt;deployment pipeline&lt;/a&gt; at 7digital and deploying changes to production is an absolute breeze. In fact, we’re so good at it we can’t stop talking about it &lt;a href=&#34;http://prezi.com/2wczo541qzpy/dddea-continuous-delivery-at-7digital/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://blogs.7digital.com/dev/2012/06/20/evolution-of-deployment-in-7digital/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://blogs.7digital.com/dev/2012/04/28/how-we-do-deployments/&#34;&gt;here&lt;/a&gt;. So what’s taking us so long to get new features out the door?&lt;/p&gt;

&lt;h2 id=&#34;technical-debt&#34;&gt;Technical Debt&lt;/h2&gt;

&lt;p&gt;We have a lot of technical debt. It’s not tested, it’s horribly coupled and entirely confusing, a proper &lt;a href=&#34;http://en.wikipedia.org/wiki/Big_ball_of_mud&#34;&gt;ball of mud&lt;/a&gt;. We try not too but occasionally we touch it and it can slow development down. We’ve been bitten before too where a change we made broke something unrelated in production. We can roll back with ease though so it’s not all that scary and can’t be entirely accountable for our tame pace.&lt;/p&gt;

&lt;h2 id=&#34;bottlenecks&#34;&gt;Bottlenecks&lt;/h2&gt;

&lt;p&gt;Developing public-facing APIs with numerous existing clients all with their own priorities and ageing implementations presents a challenge in patience and communication. Fortunately, we’ve already made some progress here: In a recent project we needed to migrate a few external API consumers to a new endpoint. We did all the work, communicated the changes and waited. Several months later we started to see some usage only to find out our endpoint wasn’t quite working as expected. To try and mitigate that we have a new policy within our team, dogfooding. We’re now attempting to consume any of our new endpoints we can. There’s two big benefits here: code running in production and failing early.&lt;/p&gt;

&lt;h2 id=&#34;unplanned-projects&#34;&gt;Unplanned Projects&lt;/h2&gt;

&lt;p&gt;Whilst looking back through the data and trying to determine project names, relationships began to emerge between occasional tasks. We found examples of sizeable unplanned projects, not just unplanned tasks, bubbling away in the background. These projects did have clear value and even related back to wider organisational priorities. As a team though we had already committed to other supposedly more important already in-progress projects. Crucially, this results in extended delivery dates on all the projects being worked on.&lt;/p&gt;

&lt;h2 id=&#34;distractions-in-parallel&#34;&gt;Distractions in Parallel&lt;/h2&gt;

&lt;p&gt;I’ve done the maths and six projects at 160 days each in under a year doesn’t add up. We’ve toyed with a number setups but generally we have two streams and therefore two projects or two features on the go at any one time with a pair on each thing (there’s four of us). This means our focus is constantly shifting between those two items whilst we attempt to give them equal attention. Looking at this further, I attempted to determine the amount of days we spent not working on an ‘in-progress’ feature and in every example it dwarfs the amount of days spent working on the feature. Clearly by spreading ourselves across two things we’re generating much more waste then we realise and unlike technical debt and bottlenecks this is one area where we have much more control.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Lean Startup - Notes</title>
      <link>https://danbadge.github.io/post/the-lean-startup-notes/</link>
      <pubDate>Mon, 17 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/the-lean-startup-notes/</guid>
      <description>

&lt;p&gt;Finally I got round to reading Eric Ries’ well known book - &lt;a href=&#34;https://www.amazon.co.uk/The-Lean-Startup-Innovation-Successful/dp/0670921602&#34;&gt;the Lean Startup&lt;/a&gt;. This book often creeps into conversations and conferences so I was keen to see what was in there rather than relying on other’s insights.&lt;/p&gt;

&lt;p&gt;From a developer point of view, it’s a fairly business-ey read as you might expect but there’s some great stuff in there and lots (and lots!) of examples from the technology industry as well as more traditional industries. Here’s my main take homes from the book:&lt;/p&gt;

&lt;h2 id=&#34;build-measure-learn&#34;&gt;Build-Measure-Learn&lt;/h2&gt;

&lt;p&gt;In the highly uncertain world of a startup you might have a good idea or a product but how do you know anybody wants it? In short; you don’t. So what can you do? Ries suggests you make an assumption of what the customer might want. Then figure out how you’re going to measure it’s success. Build it, ship it and then review the metrics. Did it work? Didn’t it? Most importantly what did we learn? The important thing is not building a truly awesome polished product; it’s learning what the customer is actually after as soon as possible. Write throw-away experiments or establish a minimal viable product to provide enough of a basis to get learning.&lt;/p&gt;

&lt;h2 id=&#34;vanity-metrics&#34;&gt;Vanity Metrics&lt;/h2&gt;

&lt;p&gt;This term was new on me and it seems to have really stuck. These are literally everywhere. Ries believes that all too often people make assumptions based on vanity metrics. A good example might be using the total number of users as validation for a product’s improvement and increased popularity. Let’s say the development team released a new user registration page on some date and there’s a spike in the total number of users, that must be because the lovely new layout, right? Possibly. Or maybe marketing started a new advertising campaign on that date? There’s probably lots of reasons all of which can’t be fully determined. This doesn’t mean vanity metrics are useless though. Using A/B Testing you could compare your vanity metrics on the two versions of the product and see which one performs better. Perhaps releasing one change for a week to half of the users to see if they’re more likely to purchase something.&lt;/p&gt;

&lt;h2 id=&#34;innovate&#34;&gt;Innovate&lt;/h2&gt;

&lt;p&gt;Companies are often formed based on an innovation. However, they soon get bogged down with the operational and infrastructural efforts which go into maintaining and improving their established product. Ries understandably argues that innovation must continue or an organisation will face stagnation. How do you successfully create an innovative environment though? Ries suggests that “strong cross-functional teams” should be allowed to develop around an innovation. These teams should have all the resources required to see a product all the way through end-to-end. Then once the product moves from phase to phase there could be a hand off between teams allowing others to take on the further improvements and maintenance. Further to this, allowing the original members to stay with that product or move on to something else. Re-reading this it actually sounds quite similar to that described in the Valve handbook for new employees. Slightly less ‘Valve’ is Ries’ vision of having ‘Entrepreneurs’ within established organisations, presumably to come up with all the ideas. Not sure about this one.&lt;/p&gt;

&lt;h2 id=&#34;waste&#34;&gt;Waste&lt;/h2&gt;

&lt;p&gt;Towards the end of the book there’s some great insight into the much wider problem of waste. Ries says “In every industry we see endless stories of failed launches, ill-conceived projects, and large-batch death spirals. I consider this misuse of people’s time a criminally negligent waste of human creativity and potential… What percentage of all that waste is preventable?”&lt;/p&gt;

&lt;p&gt;Which of our efforts are value-creating and which are wasteful? Bare in mind that value is providing benefit to the customer. How often do those of us who are software developers actually benefit the customer? Personally, as a developer of APIs and not consumer facing products, not nearly enough. Our products are abstracted away from the customer so much so that it’s very easy to lose sight of their needs.&lt;/p&gt;

&lt;p&gt;Finally, I’ll leave you with this great quote from Peter Drucker also taken from the book - “There is surely nothing quite so useless as doing with great efficiency what should not be done at all”.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programming is a bit like football, isn&#39;t it?</title>
      <link>https://danbadge.github.io/post/programming-is-like-football/</link>
      <pubDate>Sun, 19 May 2013 16:51:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/programming-is-like-football/</guid>
      <description>&lt;p&gt;The title of this blog post makes is purposefully tongue-in-cheek, let me attempt to explain. As a kid football was everything, I played a ridiculous amount. Lunchtime and after school kickabouts, perhaps training twice a week and then two matches at the weekend. It was great and academia was not so great. I was a pretty average student – not the worst, definitely not the best and certainly not interested. So naturally I grew up and became a Software Developer.&lt;/p&gt;

&lt;p&gt;What’s odd about this scene is that software development has an abundance of highly intelligent, talented people. They are thought-provoking, fascinating and challenging. Learning is at the heart of the development process and there’s an ever increasing list of books and articles to keep up with. None of which is likely to be associated with football anytime soon.&lt;/p&gt;

&lt;p&gt;I’ve been thinking a lot about why I enjoy software development and what interests me and I keep finding comparisons with football. What I’m discovering is that I’m driven by self-improvement. Each time I play football there are aspects of my game I’m trying to change. A lot of the exercise outside of football I do with a view to being fitter or stronger the next time around. Similarly with software, I’m reading recommended books, going to tech events and collaborating and challenging those around me all with the intention of being a better developer. That self-awareness to improve takes a lot of getting used to - it doesn’t come naturally and definitely isn’t easy - but if you’ve played some team sports you might have a good shot (pun intended). As it turns out this whole time I’d be running regular internal retrospectives, much like the well-known Agile practice, but with a focus on the football pitch.&lt;/p&gt;

&lt;p&gt;In a way what I’m getting at here is mastery. &lt;a href=&#34;http://www.amazon.co.uk/Drive-Surprising-Truth-About-Motivates/dp/184767769X&#34;&gt;Daniel Pink’s book Drive&lt;/a&gt; discusses mastery in great depth and is definitely worth a read. He writes “Mastery is a mindset: It requires the capacity to see your abilities not as finite, but as infinitely improveable. Mastery is a pain: It demands effort, grit and deliberate practice. And mastery is an asymptote. It’s impossible to fully realise, which makes it simultaneously frustrating and alluring.” For me this is what it’s all about, the destination might be unachievable but trying to get there is going to bloody fun.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>