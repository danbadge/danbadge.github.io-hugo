<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Watts</title>
    <link>https://danbadge.github.io/tags/technical-debt/index.xml</link>
    <description>Recent content on Daniel Watts</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://danbadge.github.io/tags/technical-debt/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Enabling change on the 7digital Catalogue</title>
      <link>https://danbadge.github.io/post/enabling-change-on-the-7digital-catalogue/</link>
      <pubDate>Tue, 26 Aug 2014 16:51:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/enabling-change-on-the-7digital-catalogue/</guid>
      <description>&lt;p&gt;Since February 2014 I’ve been working on the Content Discovery Team here at 7digital, we are responsible for every endpoint listed under Catalogue API here - &lt;a href=&#34;http://developer.7digital.com/resources/api-docs/catalogue-api&#34;&gt;http://developer.7digital.com/resources/api-docs/catalogue-api&lt;/a&gt; – and probably a few more. This is a mix of discovery endpoints and core catalogue endpoints often split by artist, albums (releases) and tracks. The discovery side of it consists of searching, recommendations, charts, genres, new and popular tracks and albums. The core catalogue endpoints are requests to the API for an album, tracks on an album, an artist, an artist’s releases or just track information using a 7digital unique identifier (referred to as releaseId, trackId and artistId).&lt;/p&gt;

&lt;p&gt;Going back to early February the team, three developers, a QA and product manager, had &lt;strong&gt;inherited a maintenance nightmare&lt;/strong&gt;. We owned around 20 endpoints across 4 applications all of which do vaguely similar things retrieving artist, release or track information. Except we were using &lt;strong&gt;two large out-of-sync data stores&lt;/strong&gt;, a SQL Server database and a Solr data store. Solr essentially containing a denormalised version of what you would find in SQL.&lt;/p&gt;

&lt;p&gt;Over the years there had been a push to use Solr as the single point of truth and there had been much investment to move all endpoints over to use it instead of SQL (a reasonable idea at the time). This meant we were using Solr for both text searching and Id lookups on most of the Catalogue endpoints but using SQL on a number of others. Furthermore we had to index all possible information relating to a track or release in Solr, much of which is territory specific. As an example; the price for an album is territory specific because it will be different in the UK to the US, as will the streaming and download release dates. In order to represent this data in a denormalised form in Solr, the solution was to &lt;strong&gt;duplicate every track and release across every territory it was available in&lt;/strong&gt;. This was a particular problem for the track index. Given we have approximately 27 million tracks in our catalogue and content in 30 odd territories, this quickly led to a massive and expensive index of tracks which was pushing &lt;strong&gt;660 GB in size and contained 700 million documents&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Due to the size of the index and the amount of data the &lt;a href=&#34;http://api.7digital.com/1.2/track/search?q=weezer&amp;amp;oauth_consumer_key=YOUR_KEY_HERE&#34;&gt;~/track/search&lt;/a&gt; endpoint performed horribly averaging 5 second response times. More pressing was the amount of &lt;strong&gt;unplanned and support-type work the team were performing in order to maintain the track index&lt;/strong&gt;. This would vary from restarting live web servers to performing manual data updates to inactivate a track, which should no longer be available on the API. The latter was common because we couldn’t perform a full re-index as it would take up to FIVE DAYS and usually failed, so we built a complex indexing application which would only send to Solr those tracks which had changed. Unfortunately this workaround was built upon an existing process which was used to identify changes in the catalogue. This consisted of tons of triggers and scheduled jobs in SQL Server where it wasn’t uncommon to see nested views, temporary tables and layers of stored procedures. &lt;strong&gt;None of it was tested and all of it was impossible to follow and reason about.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eventually a new feature came along which would have required many changes and additions at each step of this convoluted process. This gave us the business case required to begin simplifying our architecture. We recognised that we needed Solr for searching because this is exactly what it’s good at, but we saw no benefit in using it for Id lookups, particularly when we could be closer to the source data and have good performance using SQL Server. We wanted to shrink the size of the track index and so we needed to tackle a number of problems. We needed to remove territory specific data from the index so we could stop duplicating each track across 30 territories. We also needed to replace any unnecessary dependencies where possible and reduce the number of indexed fields to only those which should be searchable.&lt;/p&gt;

&lt;p&gt;To do any of this we needed to source much of the data stored in Solr from somewhere else. We recognised early on that endpoints such as ~/track/search and ~/track/chart where just a list of tracks and a full track resource could be retrieved from another public endpoint; &lt;a href=&#34;http://api.7digital.com/1.2/track/details?trackid=12345&amp;amp;oauth_consumer_key=YOUR_KEY_HERE&amp;amp;country=GB&#34;&gt;~/track/details&lt;/a&gt;. With this in mind we set about improving a number of core catalogue endpoints (including ~/track/details) reducing response times dramatically, improving the code base and bringing the monitoring of these endpoints up to scratch with everything else. The ~/track/search endpoint could then look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://danbadge.github.io/img/track-search-architecture.jpg&#34; alt=&#34;Track Search Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It remained on the old track index for a while but was only touching a few of the fields. Taking this incremental step, which had a small negative impact on performance, meant we could bring a new smaller index up and switch over to it with minimal disruption. We managed this risk further by using &lt;a href=&#34;http://www.infoq.com/news/2013/03/canary-release-improve-quality&#34;&gt;canary deploys&lt;/a&gt; too, rolling out searches on the new index to a few servers until we were satisfied enough to do a full deployment. The &lt;strong&gt;new track index weighed in at just over 6GB&lt;/strong&gt; and response times are averaging 500ms, a dramatic improvement.&lt;/p&gt;

&lt;p&gt;We can now do a &lt;strong&gt;full re-index of tracks within an hour&lt;/strong&gt;, we do this twice a day. This allows us to experiment more freely using features of Solr and Lucene without being held back by technical debt. Now we make changes daily, which allows us to test out any assumptions. This is particularly valuable when working on something as complex and subjective as text search.&lt;/p&gt;

&lt;p&gt;Crucially with the old track index gone &lt;strong&gt;unplanned work tailed off&lt;/strong&gt; and we started delivering business value. Most recently adding auto-complete style searching to all three search endpoints and resolving some long standing problems with matching on artist names like P!nk and Ke$ha.&lt;/p&gt;

&lt;p&gt;We’ve also taken all the release endpoints in a similar architectural direction, which will eventually mean we can iterate quickly on improving ~/release/search too. Technically we now have only one integration point with the catalogue database for releases and tracks, within ~/release/details and ~/track/details. Now that many of our APIs consume these endpoints we’ve improved consistency between them and we no longer have many tentacles going direct to the database, which will allow us to move a lot quicker and make improvements across the catalogue when needed.&lt;/p&gt;

&lt;p&gt;In conclusion, we benefited from a number of key concepts: we simplified and refactored when working through each of these technical changes, &lt;strong&gt;removing a lot of unnecessary complexity.&lt;/strong&gt; For example, approaching clients about removing rarely used optional parameters and in time removing them.&lt;/p&gt;

&lt;p&gt;We tackled causes of unplanned work, often eliminating any immediate burden with small fixes and workarounds so we could &lt;strong&gt;focus more of the team’s energy on the wider problem&lt;/strong&gt;. Similarly, we elevated these issues, making them visible to the rest of the business. This helped justify our business case for tackling the wider problem.&lt;/p&gt;

&lt;p&gt;We &lt;strong&gt;automated the Solr infrastructure,&lt;/strong&gt; this hasn’t been mentioned much as it’s still evolving, but we’re now able to push through configuration changes to production several times a day. This a world away from where we were and came about through a combination of configuration management (CFEngine), TDD (Test Kitchen, Vagrant), a few custom scripts to automate &lt;a href=&#34;https://wiki.apache.org/solr/CoreAdmin#RELOAD&#34;&gt;Solr core reloads&lt;/a&gt; and reducing the size of the track index.&lt;/p&gt;

&lt;p&gt;Finally, we benefited most from lots of collaboration and communication inside and outside of the team, &lt;strong&gt;discussing the technical and architectural direction&lt;/strong&gt; at length. We even started organising weekly discussions just to run through the architecture we were heading towards, any tweaks we could make or any problems we would encounter along the way. This architecture is continuing to evolve but it’s already paying off, with the team beginning to prove that we can deliver changes quickly to the 7digital catalogue.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For a more detailed and technical view on what we did and how we did it, check out Michael Okarimia’s posts: &lt;a href=&#34;http://www.michaelokarimia.com/blog/2014/08/11/fixing-7digitals-music-search-part-one-the-gargantuan-index/&#34;&gt;part one&lt;/a&gt; and &lt;a href=&#34;http://www.michaelokarimia.com/blog/2014/08/18/fixing-7digital-music-search-part-two-the-88-speed-improvement/&#34;&gt;part two&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ten deploys a day and still moving slowly</title>
      <link>https://danbadge.github.io/post/ten-deploys-a-day-and-still-moving-slowly/</link>
      <pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://danbadge.github.io/post/ten-deploys-a-day-and-still-moving-slowly/</guid>
      <description>

&lt;p&gt;There’s a feeling within my team at &lt;a href=&#34;http://www.7digital.com/&#34;&gt;7digital&lt;/a&gt; that we’re taking a long time to complete projects or features. Oddly this isn’t something we are currently measuring. We do though measure the &lt;a href=&#34;http://blog.robbowley.net/2012/01/06/productivity-throughput-and-cycle-time/&#34;&gt;throughput and cycle time&lt;/a&gt; of tasks. Tasks are items of work which the team create as a small iterative step towards a larger feature or story. This is useful for visualising the teams productivity, with the idealistic target of low cycle time and high throughput.&lt;/p&gt;

&lt;p&gt;Typically our stakeholders are interested in an estimation and delivery of a project or feature not a task. I was interested to see whether this was something we could measure, with a slant towards having historical data available for helping us estimate. I gathered all the tasks into a spreadsheet going back to the beginning of September 2012 and assigned a descriptive project and feature name I hoped the rest of the organisation could understand. This had the added benefit of forcing me to step back from the implementation and attempt to relate technical items to organisation priorities, something which is far too often overlooked.&lt;/p&gt;

&lt;p&gt;This produced some interesting results, which aligned with the team’s fears. Our average lead time of the six planned projects we had completed since September was 160 days. Over half a year! Moreover, this only represents the time it takes for an organisation-wide project to pass-through our team, often there are several teams involved each with their own changes and features to implement. We do roughly two features per project and so it’s a similar story there. On average we’re taking 72 days to complete a feature.&lt;/p&gt;

&lt;p&gt;We have our &lt;a href=&#34;http://martinfowler.com/bliki/DeploymentPipeline.html&#34;&gt;deployment pipeline&lt;/a&gt; at 7digital and deploying changes to production is an absolute breeze. In fact, we’re so good at it we can’t stop talking about it &lt;a href=&#34;http://prezi.com/2wczo541qzpy/dddea-continuous-delivery-at-7digital/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://blogs.7digital.com/dev/2012/06/20/evolution-of-deployment-in-7digital/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://blogs.7digital.com/dev/2012/04/28/how-we-do-deployments/&#34;&gt;here&lt;/a&gt;. So what’s taking us so long to get new features out the door?&lt;/p&gt;

&lt;h2 id=&#34;technical-debt&#34;&gt;Technical Debt&lt;/h2&gt;

&lt;p&gt;We have a lot of technical debt. It’s not tested, it’s horribly coupled and entirely confusing, a proper &lt;a href=&#34;http://en.wikipedia.org/wiki/Big_ball_of_mud&#34;&gt;ball of mud&lt;/a&gt;. We try not too but occasionally we touch it and it can slow development down. We’ve been bitten before too where a change we made broke something unrelated in production. We can roll back with ease though so it’s not all that scary and can’t be entirely accountable for our tame pace.&lt;/p&gt;

&lt;h2 id=&#34;bottlenecks&#34;&gt;Bottlenecks&lt;/h2&gt;

&lt;p&gt;Developing public-facing APIs with numerous existing clients all with their own priorities and ageing implementations presents a challenge in patience and communication. Fortunately, we’ve already made some progress here: In a recent project we needed to migrate a few external API consumers to a new endpoint. We did all the work, communicated the changes and waited. Several months later we started to see some usage only to find out our endpoint wasn’t quite working as expected. To try and mitigate that we have a new policy within our team, dogfooding. We’re now attempting to consume any of our new endpoints we can. There’s two big benefits here: code running in production and failing early.&lt;/p&gt;

&lt;h2 id=&#34;unplanned-projects&#34;&gt;Unplanned Projects&lt;/h2&gt;

&lt;p&gt;Whilst looking back through the data and trying to determine project names, relationships began to emerge between occasional tasks. We found examples of sizeable unplanned projects, not just unplanned tasks, bubbling away in the background. These projects did have clear value and even related back to wider organisational priorities. As a team though we had already committed to other supposedly more important already in-progress projects. Crucially, this results in extended delivery dates on all the projects being worked on.&lt;/p&gt;

&lt;h2 id=&#34;distractions-in-parallel&#34;&gt;Distractions in Parallel&lt;/h2&gt;

&lt;p&gt;I’ve done the maths and six projects at 160 days each in under a year doesn’t add up. We’ve toyed with a number setups but generally we have two streams and therefore two projects or two features on the go at any one time with a pair on each thing (there’s four of us). This means our focus is constantly shifting between those two items whilst we attempt to give them equal attention. Looking at this further, I attempted to determine the amount of days we spent not working on an ‘in-progress’ feature and in every example it dwarfs the amount of days spent working on the feature. Clearly by spreading ourselves across two things we’re generating much more waste then we realise and unlike technical debt and bottlenecks this is one area where we have much more control.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>